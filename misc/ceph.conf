[global]
admin_socket = /ceph-test/var/run/ceph/ceph-$name.$id.asok

#; allow to open a lot of files
max_open_files = 131072

#; setup logging
log_file = /ceph-test/var/log/ceph/$name.log
pid_file = /ceph-test/var/run/ceph/$name.pid

filestore_xattr_use_omap = 1

osd_pool_default_size = 1
osd_pool_default_min_size = 1

#; turn off debugs
debug_auth = 0/0
debug_asok = 0/0
debug_buffer = 0/0
debug_client = 0/0
debug_context = 0/0
debug_crush = 0/0
debug_crypto = 0/0
debug_filer = 0/0
debug_filestore = 0/0
debug_finisher = 0/0
debug_heartbeatmap = 0/0
debug_journal = 0/0
debug_journaler = 0/0
debug_lockdep = 0/0
debug_monclient = 0/0
debug_mon = 1/5
debug_monc = 1/5
debug_ms = 0/0
debug_objclass = 0/0
debug_objecter = 0/0
debug_objectcacher = 0/0
debug_optracker = 0/0
debug_osd = 0/0
debug_paxos = 0/0
debug_perfcounter = 0/0
debug_rados = 0/0
debug_rbd = 0/0
debug_rgw = 0/0
debug_timer = 0/0
debug_tp = 0/0
debug_throttle = 0/0

; debug XIO
debug_xio = 5/5 

; default datacrc & headercrc is true
ms_crc_header = false
ms_crc_data = false

; rdma setting
; no secure authentication ? A MUST for RDMA
; XioMessenger currently does not support CEPHX

auth_supported = none
auth_service_required = none
auth_client_required = none
auth_cluster_required = none
  
; IMPORTANT: This should be the local IP address of the interface that performs the RDMA (e.g. 11.11.11.1 for the server and 11.11.11.2 for the client)
rdma_local = 10.3.1.1
;rdma_local = 127.0.0.1
enable experimental unrecoverable data corrupting features = ms-type-xio
ms_type = xio
xio_mp_max_64 = 262144
xio_mp_max_256 = 262144
xio_mp_max_1k = 262144
xio_mp_max_page = 131072
xio_portal_threads = 8

osd_op_threads = 2
filesore_op_threads = 4
filestore_fd_cache_size = 64
ms_dispatch_throttle_bytes = 0

osd_op_num_threads_per_shard = 1
osd_op_num_shards = 25
filestore_fd_cache_shards = 32
throttle_perf_counter = false

rbd_cache = false

ms_bind_port_min = 17100
ms_bind_port_max = 17200

[osd]
osd_client_message_size_cap = 0
osd_client_message_cap = 0
osd_enable_op_tracker = false

osd_data = /ceph-test/ceph-data/osd.$id
osd_journal = /ceph-test/ceph-data/osd.$id/journal
osd_journal_size = 256
osd_scrub_load_threshold = 2.5
;osd_objectstore = memstore
osd_mkfs_type = xfs
osd_mount_options_xfs = rw,noatime

osd_class_dir = /opt/ceph/lib/rados-classes

[osd.0]
; Add the real hostname of the ceph server
host = set3-poc-compute001
devs = /dev/sdb
ms_bind_port_min = 7100
ms_bind_port_max = 7200
;ms bind port min = 7100
;ms bind port max = 7200






;[osd.1]
;host = set3-poc-compute003
;devs = /dev/sdb
;ms_bind_port_min = 7100
;ms_bind_port_max = 7200

; To have mkcephfs creating cluster with more OSDs on other node ex: ceph-server2

;[osd.2]
;host = set3-poc-compute005
;devs = /dev/sdb
;ms_bind_port_min = 7100
;ms_bind_port_max = 7200

;[osd.3]
;host = set3-poc-compute006
;devs = /dev/sdb
;ms_bind_port_min = 7100
;ms_bind_port_max = 7200


[mon]
mon_data = /ceph-test/ceph-data/mon.$id
debug mon = 1/5
debug paxos = 1/5
;debug auth = 2



[mon.0]
; make sure to have the ceph server hostname
host = set3-poc-compute001
mon addr = 10.3.1.1:16789
#mon_addr = 127.0.0.1:16789
;user = root


;[mon.1]
; make sure to have the ceph server hostname
;host = set3-poc-compute003
;mon_addr = 10.3.1.3:16789

;[mon.2]
; make sure to have the ceph server hostname
;host = set3-poc-compute005
;mon_addr = 10.3.1.5:16789

[mds]
; where the mds keeps its secret encryption keys
keyring = /ceph-test/ceph-data/keyring.$name
 
cluster_addr = 10.3.1.1:26789
public_addr = 10.3.1.1:36789
 
objecter_timeout = 10
mds_reconnect_timeout = 5
mds_beacon_interval = 2
 
[mds.0]
; make sure to have the ceph server hostname
host = set3-poc-compute001
;user = root

